# Path of input and output resources
input:
    train_file: lm/data/train.txt
    valid_file: lm/data/valid.txt
output:
    model_dir:       lm/output/model
    tokenizer_dir:   lm/output/tokenizer
    tensorboard_dir: lm/output/tensorboard
    checkpoint_path: lm/output/ckpt.h5

# [Model Parameters]
# For English tokenizer, specify "gpt2"
# For Japanese tokenizer, specify "cl-tohoku/bert-base-japanese"
tokenizer_model_name: "cl-tohoku/bert-base-japanese"
n_embd: 768
n_layer: 12
n_head: 12
n_ctx: 512

# [Training Parameters]
seed: 1234
num_epochs: 10
batch_size: 6  # This parameter should be equal to n_ctx
block_size: 512
learning_rate: 0.00005
max_grad_norm: 1.0
warmup_rate: 0.1
patience: 1

# [Generation Config]
do_sample: True
top_k: 50
top_p: 0.9
bad_words:
    - "[UNK]"