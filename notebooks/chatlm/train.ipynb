{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2\n",
    "\n",
    "Description here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git log -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! pip install transformers==2.6.0\n",
    "#! pip install tqdm==4.43.0\n",
    "#! pip install mecab-python3==0.996.2\n",
    "#! pip install attrdict==2.0.1\n",
    "#! pip install tensorboard==2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test library\n",
    "\n",
    "Test your all the libraries used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Declare parameters set by `papermill` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# general parameters\n",
    "name = \"model\"\n",
    "data_dir = \"notebooks/chatlm/data_sample\"\n",
    "pretrained_dir = \"notebooks/gpt/output/model\"\n",
    "output_dir =\"output\"\n",
    "\n",
    "# training parameters\n",
    "seed=1234\n",
    "num_epochs=10\n",
    "batch_size=2\n",
    "learning_rate=5e-5\n",
    "max_grad_norm=1.0\n",
    "warmup_rate=0.1\n",
    "patience = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attrdict\n",
    "\n",
    "_params = attrdict.AttrDict({\n",
    "    \"name\": name,\n",
    "    \"data_dir\": data_dir,\n",
    "    \"pretrained_dir\": pretrained_dir,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"seed\": seed,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"max_grad_norm\": max_grad_norm,\n",
    "    \"warmup_rate\": warmup_rate,\n",
    "    \"patience\": patience,\n",
    "})\n",
    "\n",
    "del name\n",
    "del data_dir\n",
    "del pretrained_dir\n",
    "del output_dir\n",
    "del seed\n",
    "del num_epochs\n",
    "del batch_size\n",
    "del learning_rate\n",
    "del max_grad_norm\n",
    "del warmup_rate\n",
    "del patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define preprocessor, tokenizer and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and save vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "_model_output_dir = os.path.join(_params.output_dir, _params.name)\n",
    "if not os.path.exists(_model_output_dir):\n",
    "    os.mkdir(_model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptchat.lib.chatlm import ChatLMTokenizerBuilder\n",
    "\n",
    "_tokenizer = ChatLMTokenizerBuilder().build()\n",
    "_tokenizer.save_pretrained(_model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # When use GPU\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gptchat.lib.chatlm import ChatLMDataloaderBuilder\n",
    "from gptchat.lib.chatlm import ChatLMDataset\n",
    "from gptchat.lib.chatlm import ChatLMModelBuilder\n",
    "from gptchat.lib.trainer import Trainer\n",
    "\n",
    "\n",
    "def get_texts(filepath):\n",
    "    return [line.strip(\"\\n\").split(\"\\t\") for line in open(filepath)]\n",
    "\n",
    "\n",
    "def train(model_output_dir, params):\n",
    "    # Fix seed for reproducability\n",
    "    set_seed(seed=params.seed)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = transformers.BertJapaneseTokenizer.from_pretrained(model_output_dir)\n",
    "    \n",
    "    # Build dataloader\n",
    "    train_data = get_texts(params.data_dir + \"/train.tsv\")\n",
    "    valid_data = get_texts(params.data_dir + \"/valid.tsv\")\n",
    "    dataloader_builder = ChatLMDataloaderBuilder()\n",
    "    dataloader_dict = {\n",
    "        \"train\": ChatLMDataloaderBuilder().build(\n",
    "            dataset=ChatLMDataset(tokenizer, train_data),\n",
    "            batch_size=params.batch_size,\n",
    "            shuffle=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        ),\n",
    "        \"val\": ChatLMDataloaderBuilder().build(\n",
    "            dataset=ChatLMDataset(tokenizer, valid_data),\n",
    "            batch_size=params.batch_size,\n",
    "            shuffle=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    # Model\n",
    "    net = ChatLMModelBuilder().from_pretrained(\n",
    "        pretrained_dir=params.pretrained_dir,\n",
    "        vocab_size=len(tokenizer)\n",
    "    )\n",
    "\n",
    "    # create train config\n",
    "    optimizer = torch.optim.Adam(net.parameters(),  lr=params.learning_rate)\n",
    "    total_steps = len(dataloader_dict[\"train\"]) * params.num_epochs\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps*params.warmup_rate,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model_output_dir=model_output_dir,\n",
    "        net=net,\n",
    "        dataloader_dict=dataloader_dict,\n",
    "        num_epochs=params.num_epochs,\n",
    "        device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        max_grad_norm=params.max_grad_norm,\n",
    "        patience=params.patience,\n",
    "        writer=SummaryWriter(log_dir=params.output_dir + \"/runs/\" + params.name),\n",
    "        tqdm_disable=True,\n",
    "    )\n",
    "    \n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(_model_output_dir, _params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_output_dir, params):\n",
    "    # Load model\n",
    "    tokenizer = transformers.BertJapaneseTokenizer.from_pretrained(model_output_dir)\n",
    "    net = transformers.GPT2LMHeadModel.from_pretrained(model_output_dir)\n",
    "\n",
    "    # Build dataloader\n",
    "    valid_data = get_texts(params.data_dir + \"/test.tsv\")\n",
    "    dataloader_builder = ChatLMDataloaderBuilder()\n",
    "    dataloader_dict = {\n",
    "        \"val\": ChatLMDataloaderBuilder().build(\n",
    "            dataset=ChatLMDataset(tokenizer, valid_data),\n",
    "            batch_size=params.batch_size,\n",
    "            shuffle=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model_output_dir=None,\n",
    "        net=net,\n",
    "        dataloader_dict=dataloader_dict,\n",
    "        num_epochs=0,\n",
    "        device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        max_grad_norm=None,\n",
    "        patience=1,\n",
    "        writer=None,\n",
    "        tqdm_disable=True,\n",
    "    )\n",
    "    \n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(_model_output_dir, _params)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
